--- include/linux/sched.h	2013-01-04 04:01:30.000000000 -0800
+++ include/linux/sched.h	2013-03-19 04:00:51.000000000 -0700
@@ -91,31 +81,17 @@
 struct perf_event_context;
 struct blk_plug;
 
-/*
- * List of flags we want to share for kernel threads,
- * if only because they are not used by them anyway.
- */
 #define CLONE_KERNEL	(CLONE_FS | CLONE_FILES | CLONE_SIGHAND)
 
-/*
- * These are the constant used to fake the fixed-point load-average
- * counting. Some notes:
- *  - 11 bit fractions expand to 22 bits by the multiplies: this gives
- *    a load-average precision of 10 bits integer + 11 bits fractional
- *  - if you want to count load-averages more often, you need more
- *    precision, or rounding will get you. With 2-second counting freq,
- *    the EXP_n values would be 1981, 2034 and 2043 if still using only
- *    11 bit fractions.
- */
-extern unsigned long avenrun[];		/* Load averages */
+extern unsigned long avenrun[];		
 extern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);
 
-#define FSHIFT		11		/* nr of bits of precision */
-#define FIXED_1		(1<<FSHIFT)	/* 1.0 as fixed-point */
-#define LOAD_FREQ	(5*HZ+1)	/* 5 sec intervals */
-#define EXP_1		1884		/* 1/exp(5sec/1min) as fixed-point */
-#define EXP_5		2014		/* 1/exp(5sec/5min) */
-#define EXP_15		2037		/* 1/exp(5sec/15min) */
+#define FSHIFT		11		
+#define FIXED_1		(1<<FSHIFT)	
+#define LOAD_FREQ	(5*HZ+1)	
+#define EXP_1		1884		
+#define EXP_5		2014		
+#define EXP_15		2037		
 
 #define CALC_LOAD(load,exp,n) \
 	load *= exp; \
@@ -734,33 +488,33 @@
 extern int sched_domain_level_max;
 
 struct sched_domain {
-	/* These fields must be setup */
-	struct sched_domain *parent;	/* top domain must be null terminated */
-	struct sched_domain *child;	/* bottom domain must be null terminated */
-	struct sched_group *groups;	/* the balancing groups of the domain */
-	unsigned long min_interval;	/* Minimum balance interval ms */
-	unsigned long max_interval;	/* Maximum balance interval ms */
-	unsigned int busy_factor;	/* less balancing by factor if busy */
-	unsigned int imbalance_pct;	/* No balance until over watermark */
-	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
+	
+	struct sched_domain *parent;	
+	struct sched_domain *child;	
+	struct sched_group *groups;	
+	unsigned long min_interval;	
+	unsigned long max_interval;	
+	unsigned int busy_factor;	
+	unsigned int imbalance_pct;	
+	unsigned int cache_nice_tries;	
 	unsigned int busy_idx;
 	unsigned int idle_idx;
 	unsigned int newidle_idx;
 	unsigned int wake_idx;
 	unsigned int forkexec_idx;
 	unsigned int smt_gain;
-	int flags;			/* See SD_* */
+	int flags;			
 	int level;
 
-	/* Runtime fields. */
-	unsigned long last_balance;	/* init to jiffies. units in jiffies */
-	unsigned int balance_interval;	/* initialise to 1. units in ms. */
-	unsigned int nr_balance_failed; /* initialise to 0 */
+	
+	unsigned long last_balance;	
+	unsigned int balance_interval;	
+	unsigned int nr_balance_failed; 
 
 	u64 last_update;
 
 #ifdef CONFIG_SCHEDSTATS
-	/* load_balance() stats */
+	
 	unsigned int lb_count[CPU_MAX_IDLE_TYPES];
 	unsigned int lb_failed[CPU_MAX_IDLE_TYPES];
 	unsigned int lb_balanced[CPU_MAX_IDLE_TYPES];
@@ -1871,19 +1429,10 @@
 		__mmdrop(mm);
 }
 
-/* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);
-/* Grab a reference to a task's mm, if it is not already going away */
 extern struct mm_struct *get_task_mm(struct task_struct *task);
-/*
- * Grab a reference to a task's mm, if it is not already going away
- * and ptrace_may_access with the mode parameter passed to it
- * succeeds.
- */
 extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
-/* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
-/* Allocate a new mm structure and copy contents from tsk->mm */
 extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
 extern int copy_thread(unsigned long, unsigned long, unsigned long,
@@ -2190,20 +1671,11 @@
 	raw_spin_lock_init(&sig->cputimer.lock);
 }
 
-/*
- * Reevaluate whether the task has signals pending delivery.
- * Wake the task if so.
- * This is required every time the blocked sigset_t changes.
- * callers must hold sighand->siglock.
- */
 extern void recalc_sigpending_and_wake(struct task_struct *t);
 extern void recalc_sigpending(void);
 
 extern void signal_wake_up(struct task_struct *t, int resume_stopped);
 
-/*
- * Wrappers for p->thread_info->cpu access. No-op on UP.
- */
 #ifdef CONFIG_SMP
 
 static inline unsigned int task_cpu(const struct task_struct *p)
