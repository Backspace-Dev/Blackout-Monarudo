--- net/ipv4/tcp_input.c	2013-01-04 04:01:31.000000000 -0800
+++ net/ipv4/tcp_input.c	2013-03-19 04:00:51.000000000 -0700
@@ -1715,8 +1160,6 @@
 		tcp_reset_reno_sack(tp);
 
 	if (!how) {
-		/* Push undo marker, if it was plain RTO and nothing
-		 * was retransmitted. */
 		tp->undo_marker = tp->snd_una;
 	} else {
 		tp->sacked_out = 0;
@@ -2780,7 +1903,7 @@
 		}
 	} else {
 		if (!(flag & FLAG_DATA_ACKED) && (tp->frto_counter == 1)) {
-			/* Prevent sending of new data. */
+			
 			tp->snd_cwnd = min(tp->snd_cwnd,
 					   tcp_packets_in_flight(tp));
 			return 1;
@@ -2844,15 +1966,9 @@
 	int newly_acked_sacked = 0;
 	int frto_cwnd = 0;
 
-	/* If the ack is older than previous acks
-	 * then we can probably ignore it.
-	 */
 	if (before(ack, prior_snd_una))
 		goto old_ack;
 
-	/* If the ack includes data we haven't sent yet, discard
-	 * this segment (RFC793 Section 3.9).
-	 */
 	if (after(ack, tp->snd_nxt))
 		goto invalid_ack;
 
@@ -2935,14 +2044,10 @@
 	return 1;
 
 no_queue:
-	/* If data was DSACKed, see if we can undo a cwnd reduction. */
+	
 	if (flag & FLAG_DSACKING_ACK)
 		tcp_fastretrans_alert(sk, pkts_acked, newly_acked_sacked,
 				      is_dupack, flag);
-	/* If this ack opens up a zero window, clear backoff.  It was
-	 * being used to time the probes, and is probably far higher than
-	 * it needs to be for normal retransmission.
-	 */
 	if (tcp_send_head(sk))
 		tcp_ack_probe(sk);
 	return 1;
@@ -3400,21 +2399,21 @@
 	int num_sacks = tp->rx_opt.num_sacks;
 	int this_sack;
 
-	/* Empty ofo queue, hence, all the SACKs are eaten. Clear. */
+	
 	if (skb_queue_empty(&tp->out_of_order_queue)) {
 		tp->rx_opt.num_sacks = 0;
 		return;
 	}
 
 	for (this_sack = 0; this_sack < num_sacks;) {
-		/* Check if the start of the sack is covered by RCV.NXT. */
+		
 		if (!before(tp->rcv_nxt, sp->start_seq)) {
 			int i;
 
-			/* RCV.NXT must cover all the block! */
+			
 			WARN_ON(before(tp->rcv_nxt, sp->end_seq));
 
-			/* Zap this SACK, by moving forward any other SACKS. */
+			
 			for (i=this_sack+1; i < num_sacks; i++)
 				tp->selective_acks[i-1] = tp->selective_acks[i];
 			num_sacks--;
@@ -4183,18 +3068,15 @@
 out:
 	return copied_early;
 }
-#endif /* CONFIG_NET_DMA */
+#endif 
 
-/* Does PAWS and seqno based validation of an incoming segment, flags will
- * play significant role here.
- */
 static int tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,
 			      const struct tcphdr *th, int syn_inerr)
 {
 	const u8 *hash_location;
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	/* RFC1323: H1. Apply PAWS check first. */
+	
 	if (tcp_fast_parse_options(skb, th, tp, &hash_location) &&
 	    tp->rx_opt.saw_tstamp &&
 	    tcp_paws_discard(sk, skb)) {
@@ -4203,36 +3085,27 @@
 			tcp_send_dupack(sk, skb);
 			goto discard;
 		}
-		/* Reset is accepted even if it did not pass PAWS. */
+		
 	}
 
-	/* Step 1: check sequence number */
+	
 	if (!tcp_sequence(tp, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq)) {
-		/* RFC793, page 37: "In all states except SYN-SENT, all reset
-		 * (RST) segments are validated by checking their SEQ-fields."
-		 * And page 69: "If an incoming segment is not acceptable,
-		 * an acknowledgment should be sent in reply (unless the RST
-		 * bit is set, if so drop the segment and return)".
-		 */
 		if (!th->rst)
 			tcp_send_dupack(sk, skb);
 		goto discard;
 	}
 
-	/* Step 2: check RST bit */
+	
 	if (th->rst) {
 		tcp_reset(sk);
 		goto discard;
 	}
 
-	/* ts_recent update must be made after we are sure that the packet
-	 * is in window.
-	 */
 	tcp_replace_ts_recent(tp, TCP_SKB_CB(skb)->seq);
 
-	/* step 3: check security and precedence [ignored] */
+	
 
-	/* step 4: Check for a SYN in window. */
+	
 	if (th->syn && !before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {
 		if (syn_inerr)
 			TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
@@ -4248,108 +3121,47 @@
 	return 0;
 }
 
-/*
- *	TCP receive function for the ESTABLISHED state.
- *
- *	It is split into a fast path and a slow path. The fast path is
- * 	disabled when:
- *	- A zero window was announced from us - zero window probing
- *        is only handled properly in the slow path.
- *	- Out of order segments arrived.
- *	- Urgent data is expected.
- *	- There is no buffer space left
- *	- Unexpected TCP flags/window values/header lengths are received
- *	  (detected by checking the TCP header against pred_flags)
- *	- Data is sent in both directions. Fast path only supports pure senders
- *	  or pure receivers (this means either the sequence number or the ack
- *	  value must stay constant)
- *	- Unexpected TCP option.
- *
- *	When these conditions are not satisfied it drops into a standard
- *	receive procedure patterned after RFC793 to handle all cases.
- *	The first three cases are guaranteed by proper pred_flags setting,
- *	the rest is checked inline. Fast processing is turned on in
- *	tcp_data_queue when everything is OK.
- */
 int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			const struct tcphdr *th, unsigned int len)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	int res;
 
-	/*
-	 *	Header prediction.
-	 *	The code loosely follows the one in the famous
-	 *	"30 instruction TCP receive" Van Jacobson mail.
-	 *
-	 *	Van's trick is to deposit buffers into socket queue
-	 *	on a device interrupt, to call tcp_recv function
-	 *	on the receive process context and checksum and copy
-	 *	the buffer to user space. smart...
-	 *
-	 *	Our current scheme is not silly either but we take the
-	 *	extra cost of the net_bh soft interrupt processing...
-	 *	We do checksum and copy also but from device to kernel.
-	 */
 
 	tp->rx_opt.saw_tstamp = 0;
 
-	/*	pred_flags is 0xS?10 << 16 + snd_wnd
-	 *	if header_prediction is to be made
-	 *	'S' will always be tp->tcp_header_len >> 2
-	 *	'?' will be 0 for the fast path, otherwise pred_flags is 0 to
-	 *  turn it off	(when there are holes in the receive
-	 *	 space for instance)
-	 *	PSH flag is ignored.
-	 */
 
 	if ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&
 	    TCP_SKB_CB(skb)->seq == tp->rcv_nxt &&
 	    !after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {
 		int tcp_header_len = tp->tcp_header_len;
 
-		/* Timestamp header prediction: tcp_header_len
-		 * is automatically equal to th->doff*4 due to pred_flags
-		 * match.
-		 */
 
-		/* Check timestamp */
+		
 		if (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {
-			/* No? Slow path! */
+			
 			if (!tcp_parse_aligned_timestamp(tp, th))
 				goto slow_path;
 
-			/* If PAWS failed, check it more carefully in slow path */
+			
 			if ((s32)(tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent) < 0)
 				goto slow_path;
 
-			/* DO NOT update ts_recent here, if checksum fails
-			 * and timestamp was corrupted part, it will result
-			 * in a hung connection since we will drop all
-			 * future packets due to the PAWS test.
-			 */
 		}
 
 		if (len <= tcp_header_len) {
-			/* Bulk data transfer: sender */
+			
 			if (len == tcp_header_len) {
-				/* Predicted packet is in window by definition.
-				 * seq == rcv_nxt and rcv_wup <= rcv_nxt.
-				 * Hence, check seq<=rcv_wup reduces to:
-				 */
 				if (tcp_header_len ==
 				    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&
 				    tp->rcv_nxt == tp->rcv_wup)
 					tcp_store_ts_recent(tp);
 
-				/* We know that such packets are checksummed
-				 * on entry.
-				 */
 				tcp_ack(sk, skb, 0);
 				__kfree_skb(skb);
 				tcp_data_snd_check(sk);
 				return 0;
-			} else { /* Header too small */
+			} else { 
 				TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
 				goto discard;
 			}
